{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "import re\n",
    "import keras\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, GlobalAveragePooling1D, concatenate\n",
    "from keras.models import Model, Sequential\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.callbacks import CSVLogger, ReduceLROnPlateau, ModelCheckpoint "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the csv files\n",
    "fashion_df = pd.read_csv(\"./datasets/fashion_text_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sless Flippy Mini Dress</td>\n",
       "      <td>women dresses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Midi Pleated Skirt</td>\n",
       "      <td>women skirts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stamos Mix Silk Raglan Knit Sweater</td>\n",
       "      <td>men outwear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basic Training Tank Top</td>\n",
       "      <td>men tops</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fashion Camouflage Hooded Jacket</td>\n",
       "      <td>women outwear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Straight Leg Sweatpants</td>\n",
       "      <td>men trousers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Hooded Pull Over</td>\n",
       "      <td>men outwear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ally 3/4 Spring Pullover</td>\n",
       "      <td>women outwear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Cropped Pleated Pants</td>\n",
       "      <td>women trousers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NB Athletics Archive Run Pants</td>\n",
       "      <td>men trousers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           description        category\n",
       "0              Sless Flippy Mini Dress   women dresses\n",
       "1                   Midi Pleated Skirt    women skirts\n",
       "2  Stamos Mix Silk Raglan Knit Sweater     men outwear\n",
       "3              Basic Training Tank Top        men tops\n",
       "4     Fashion Camouflage Hooded Jacket   women outwear\n",
       "5              Straight Leg Sweatpants    men trousers\n",
       "6                     Hooded Pull Over     men outwear\n",
       "7             Ally 3/4 Spring Pullover   women outwear\n",
       "8                Cropped Pleated Pants  women trousers\n",
       "9       NB Athletics Archive Run Pants    men trousers"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Showing the first few columns of the dataset\n",
    "fashion_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "description    2000\n",
       "category       2000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fashion_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 1538)\n"
     ]
    }
   ],
   "source": [
    "docs = map(''.join, fashion_df[['description']].values.tolist())\n",
    "\n",
    "cv = CountVectorizer(stop_words=\"english\", \n",
    "                        analyzer='word', \n",
    "                        ngram_range=(1, 1), \n",
    "                        max_df=1.0, min_df=1, \n",
    "                        max_features=None)\n",
    "\n",
    "\n",
    "# Generates word count for the words in the docs\n",
    "word_count_vector = cv.fit_transform(docs)\n",
    "print(word_count_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get more insights from IDF & TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>shorts</th>\n",
       "      <td>3.060139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skirt</th>\n",
       "      <td>3.240110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dress</th>\n",
       "      <td>3.273526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shirt</th>\n",
       "      <td>3.313135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sleeve</th>\n",
       "      <td>3.624669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pants</th>\n",
       "      <td>3.681421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jacket</th>\n",
       "      <td>3.757215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>midi</th>\n",
       "      <td>3.957011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fit</th>\n",
       "      <td>3.966673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>short</th>\n",
       "      <td>4.016435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             idf\n",
       "shorts  3.060139\n",
       "skirt   3.240110\n",
       "dress   3.273526\n",
       "shirt   3.313135\n",
       "sleeve  3.624669\n",
       "pants   3.681421\n",
       "jacket  3.757215\n",
       "midi    3.957011\n",
       "fit     3.966673\n",
       "short   4.016435"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---  IDF  ---\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)\n",
    " \n",
    "# idf values\n",
    "df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names(),columns=[\"idf\"])\n",
    " \n",
    "# sort ascending\n",
    "df_idf = df_idf.sort_values(by=['idf'])\n",
    "df_idf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sless</th>\n",
       "      <td>0.640754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flippy</th>\n",
       "      <td>0.607902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mini</th>\n",
       "      <td>0.386706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dress</th>\n",
       "      <td>0.265232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0002</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prem</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prairie</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>power</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>postgame</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>popover</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tfidf\n",
       "sless     0.640754\n",
       "flippy    0.607902\n",
       "mini      0.386706\n",
       "dress     0.265232\n",
       "0002      0.000000\n",
       "prem      0.000000\n",
       "prairie   0.000000\n",
       "power     0.000000\n",
       "postgame  0.000000\n",
       "popover   0.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---  TFIDF  ---\n",
    "# tf-idf scores\n",
    "tf_idf_vector=tfidf_transformer.transform(word_count_vector)\n",
    "\n",
    "feature_names = cv.get_feature_names()\n",
    " \n",
    "#get tfidf vector for first document\n",
    "first_document_vector=tf_idf_vector[0]\n",
    " \n",
    "#print the scores\n",
    "df = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"])\n",
    "df = df.sort_values(by=[\"tfidf\"],ascending=False)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocab:  1538\n",
      "Sample of vocab ['0002', '0004', '0005', '0006', '0007', '0011', '0012', '0016', '0096', '0175', '0241', '026', '035', '056', '10k', '119', '129', '18882', '18883', '18884', '19', '1979', '237', '28', '2c', '2pcs', '2x', '30', '3031', '3060', '360', '36272', '37', '3d', '3s', '430', '4g', '50', '505', '511', '52797', '54', '59406', '59407', '59463', '596346', '5palle', '60', '66', '705', '712', '720', '721', '724', '779', '78', '790', '79488', '79697', '90', '90s', '93308w', '93343', '93368w', '93417w', 'a003', 'a249', 'ab', 'abrig', 'abstract', 'ac', 'accent', 'accordian', 'acid', 'action', 'activchill', 'active', 'ad', 'adda', 'ader', 'adidas', 'adjustable', 'adrian', 'aery', 'aiden', 'air', 'aja', 'alchemy', 'alex', 'alice', 'ally', 'aloha', 'altitude', 'alvia', 'amari', 'amelie', 'american', 'amnig', 'ams', 'animal', 'ankle', 'ann', 'anorak', 'anthoney', 'antonia', 'aop', 'août', 'applique', 'ar', 'arch', 'archive', 'arctic', 'arden', 'armour', 'army', 'asia', 'asymmetric', 'asymmetrical', 'athleisure', 'athlete', 'athletic', 'athletics', 'attacker', 'aubergine', 'authentic', 'autumn', 'autumnal', 'axis', 'baby', 'babydoll', 'badge', 'bag', 'balboa', 'ball', 'baloon', 'band', 'bardot', 'bare', 'barnett', 'barney', 'barre', 'barro', 'baseball', 'baseline', 'basic', 'batwing', 'bay', 'bea', 'beach', 'beaded', 'bear', 'bedford', 'beige', 'bellista', 'belt', 'belted', 'ben', 'bergman', 'bermuda', 'bermudas', 'bessin', 'bestwick', 'bf', 'bias', 'bicolor', 'bicycle', 'big', 'biker', 'billboard', 'billie', 'billy', 'binea', 'birdie', 'biter', 'black', 'blake', 'blauw', 'blazer', 'blend', 'blended', 'blinders', 'block', 'blocking', 'blossom', 'blouse', 'blouses', 'blowout', 'blue', 'blush', 'boardshorts', 'boardy', 'boat', 'bodega', 'body', 'bodycon', 'bodysuit', 'boho', 'bolt', 'bomb', 'bomber', 'booty', 'borg', 'bos', 'boss', 'bossini', 'bottoms', 'boucle', 'bow', 'box', 'boxy', 'boy', 'boyfriend', 'brad', 'brand', 'breast', 'breasted', 'breathylon', 'brenda', 'brett', 'bright', 'british', 'britney', 'broadley', 'brody', 'brown', 'brushed', 'bt', 'bts', 'buckle', 'buffalo', 'bull', 'bum', 'burgundy', 'burnout', 'button', 'buttoned', 'buttons', 'cable', 'cahne', 'caleb', 'calf', 'cali', 'california', 'calvin', 'camel', 'cami', 'camo', 'camoflouge', 'camouflage', 'camp', 'canaletto', 'canvas', 'cap', 'cape', 'capri', 'capris', 'cara', 'cardigan', 'cargo', 'carnival', 'carter', 'cartter', 'cascade', 'cassandra', 'castro', 'casual', 'catelyn', 'cedar', 'celebration', 'celestina', 'celsius', 'center', 'cest', 'chai', 'chain', 'chambray', 'champion', 'charcoal', 'charged', 'charlize', 'check', 'checkboard', 'checked', 'checkered', 'cheetah', 'chest', 'chevron', 'chic', 'chiffon', 'chino', 'chinos', 'choc', 'chris', 'chrissy', 'christmas', 'cigarette', 'cindy', 'circle', 'city', 'ck', 'ckj', 'cl', 'clark', 'clash', 'classic', 'classics', 'cleo', 'cliff', 'climachill', 'clive', 'closet', 'cloud', 'club', 'cny', 'coast', 'coat', 'coated', 'cobra', 'cold', 'collar', 'collared', 'collarless', 'collection', 'collections', 'collier', 'collin', 'color', 'colorblock', 'colour', 'colourblock', 'coloured', 'comfort', 'comfortable', 'concealed', 'conner', 'contrast', 'converse', 'cool', 'coraille', 'cord', 'corduroy', 'core', 'cornelia', 'corp', 'corset', 'cotton', 'county', 'cover', 'cozy', 'cp', 'crafted', 'crazy', 'cream', 'crepe', 'crescent', 'crest', 'crew', 'crewneck', 'crinkle', 'crocodile', 'crombie', 'crop', 'cropped', 'cross', 'crossed', 'crystal', 'cuff', 'cuffed', 'culotte', 'culottes', 'curve', 'curved', 'cut', 'cycle', 'cycling', 'dad', 'daffy', 'daisy', 'dance', 'dark', 'day', 'dean', 'debora', 'defacto', 'degrade', 'delaware3', 'denim', 'denizen', 'density', 'departure', 'desert', 'destroyed', 'detachable', 'detailed', 'details', 'dial', 'diamante', 'diecut', 'dillon', 'dime', 'dip', 'distressed', 'ditsy', 'divine', 'dobby', 'doccer', 'dockers', 'doffy', 'dogtooth', 'doll', 'donovan', 'dot', 'double', 'downhill', 'dp', 'drape', 'draped', 'drawstring', 'dress', 'dresses', 'dresss', 'drop', 'dropped', 'dry', 'drying', 'dua', 'dual', 'duke', 'dunk', 'duster', 'dye', 'early', 'earn', 'easy', 'eclipse', 'eco', 'ecru', 'eden', 'edge', 'edges', 'edging', 'edit', 'edition', 'elastic', 'elasticised', 'elegant', 'element', 'eletta', 'elite', 'ellesse', 'emalia', 'emblem', 'emboss', 'embossed', 'embroidered', 'embroidery', 'emilie', 'emma', 'emmalyn', 'emsie', 'end', 'entry', 'epic', 'era', 'eric', 'error', 'ervina', 'espresso', 'ess', 'essential', 'essentials', 'etta', 'evening', 'exposed', 'extended', 'extensions', 'eyelet', 'eyez', 'fabric', 'face', 'fade', 'fake', 'famme', 'fang', 'fantasy', 'fashion', 'fast', 'faux', 'favorite', 'favour', 'fbt', 'field', 'fight', 'fila', 'fine', 'finsen', 'fireball', 'fish', 'fishtail', 'fishtale', 'fit', 'fitness', 'fitted', 'fl_spr', 'flag', 'flamestrk', 'flannel', 'flap', 'flare', 'flared', 'flashback', 'flat', 'fleck', 'fleece', 'flex', 'flexx', 'flippy', 'flirt', 'flmp', 'floor', 'floral', 'flores', 'flounce', 'flower', 'flowy', 'fluffy', 'flute', 'fluted', 'flutter', 'fly', 'flyer', 'foil', 'fold', 'folded', 'foldover', 'foliage', 'font', 'foot', 'form', 'foundation', 'frankie', 'frayed', 'freelift', 'freelift_sport', 'french', 'frill', 'frilled', 'fringe', 'fringed', 'frost', 'fruit', 'fubar', 'fuji', 'fulton', 'functional', 'funnel', 'fur', 'fusion', 'fz', 'gabardine', 'gabbi', 'galactic', 'galaxy', 'gathered', 'gauche', 'gauzy', 'geo', 'geogina', 'geometric', 'george', 'georgia', 'gfx', 'gift', 'gilet', 'gingham', 'ginny', 'glam', 'glass', 'glitter', 'globe', 'glory', 'gold', 'goldie', 'goods', 'gordon', 'gored', 'gps', 'gradient', 'grand', 'grandad', 'grant', 'graphic', 'graphics', 'grazer', 'green', 'grey', 'grit', 'guess', 'gworld', 'gym', 'half', 'halfrida', 'halter', 'hamilton', 'handkerchief', 'hannon', 'hansen', 'harden', 'harlyn', 'harmony', 'harrington', 'harriotte', 'haves', 'hayden', 'hayley', 'hd', 'hea', 'heart', 'heatgear', 'heather', 'heathertech', 'heavy', 'hello', 'hem', 'henley', 'heritage', 'hg', 'hi', 'high', 'highlight', 'hiker', 'himba782ovd', 'hoddie', 'hoff', 'holmen', 'hood', 'hooded', 'hoodie', 'hoodies', 'hoody', 'hopkins', 'hot', 'houndstooth', 'house', 'housemark', 'hr', 'hunter', 'hybrid', 'hynda', 'hz7022', 'hz7023', 'i581083', 'ice', 'icon', 'iconic', 'id', 'ignition', 'ii', 'iii', 'ikuno', 'inch', 'inches', 'indie', 'indigo', 'inez', 'infill', 'informal', 'inner', 'insert', 'inserts', 'institutional', 'int', 'intdaisyprintculotte', 'interlock', 'international', 'inverted', 'iron', 'irregular', 'irving', 'isn', 'ivory', 'jacey', 'jacket', 'jackets', 'jacob', 'jacquard', 'jacqueline', 'james', 'jamie', 'japan', 'jared', 'jaylah', 'jdi', 'jeans', 'jeff', 'jegging', 'jeggings', 'jenn', 'jenna', 'jennifer', 'jers', 'jersey', 'joey', 'jog', 'jogger', 'joggers', 'jogging', 'john', 'jorbuds', 'jorcall', 'joshua', 'joy', 'jubah', 'judas', 'jude', 'jumper', 'jupiter', 'k208', 'kaiya', 'kay', 'kelly', 'kendall', 'kendra', 'kenzie', 'khaki', 'khakis', 'kickflare', 'kimono', 'kitty', 'klein', 'knee', 'knit', 'knitted', 'knitwear', 'knot', 'knotted', 'korea', 'korean', 'kurung', 'kyoto', 'la', 'label', 'lace', 'laguna', 'lapel', 'lapelled', 'laser', 'launch', 'laurel', 'layback', 'layer', 'layered', 'layers', 'lazlo', 'leaf', 'leather', 'left', 'leg', 'legend', 'legged', 'legging', 'leggings', 'leigh', 'lella', 'lemon', 'length', 'leo', 'leopard', 'lettuce', 'levi', 'lewis', 'liam', 'life', 'lifesweatshirt', 'lifting', 'light', 'lightweight', 'like', 'limited', 'line', 'linear', 'lined', 'linen', 'lines', 'lining', 'lion', 'lipa', 'lisa', 'lite', 'lm', 'lo', 'lock', 'logo', 'logocon', 'lola', 'long', 'longline', 'loop', 'loose', 'lottie', 'lotus', 'low', 'ls', 'lucas', 'luke', 'lurex', 'lux', 'luxe', 'm17', 'm20', 'macchiato', 'maddy', 'magnet', 'maison', 'maisy', 'man', 'manager', 'mandarin', 'manipulate', 'manipulated', 'marco', 'mark', 'marl', 'marlene', 'massha', 'matcha', 'matching', 'mateo', 'material', 'maternity', 'mathieu', 'matteo', 'maverick', 'maxi', 'media', 'medium', 'meet', 'meg', 'mehru', 'mel', 'melange', 'men', 'mennace', 'mens', 'menswear', 'mermaid', 'mesh', 'metallic', 'mexico', 'mh', 'mialle', 'michelle', 'micro', 'microfibre', 'microprint', 'mid', 'midaxi', 'midi', 'midwash', 'midweight', 'mikkola', 'miler', 'military', 'miller', 'mina', 'mini', 'minimal', 'mint', 'mirage', 'mirna', 'missguided', 'misty', 'mix', 'mixed', 'mk', 'mk1', 'mock', 'modern', 'mom', 'mono', 'monochrome', 'monogram', 'montana', 'morgan', 'mori', 'morris', 'mosaic', 'moto', 'msgd', 'multi', 'muscle', 'mustard', 'mylolind', 'nadare', 'nautical', 'navy', 'nb', 'neck', 'needle', 'nelson', 'net', 'new', 'newport', 'nhiz', 'nike', 'non', 'norah', 'normal', 'nova', 'novelty', 'nude', 'nutcracker', 'nyc', 'obey', 'oceanic', 'ochre', 'offside', 'og', 'ol', 'olive', 'ombre', 'onsmike', 'onsrain', 'op', 'open', 'openwork', 'optiks', 'orange', 'ord', 'ords', 'organic', 'original', 'originale', 'originals', 'ost', 'outer', 'outline', 'overall', 'overcast', 'overcoat', 'overdye', 'overhead', 'overlap', 'overlay', 'overshirt', 'oversize', 'oversized', 'oxford', 'pacey', 'pack', 'packable', 'padded', 'painted', 'paisley', 'pajama', 'palazzo', 'pale', 'pallas', 'palm', 'pamela', 'panel', 'paneled', 'panelled', 'pant', 'pants', 'paper', 'paperbag', 'parachute', 'pareo', 'paris', 'parka', 'parlay', 'patch', 'pattern', 'patterned', 'peace', 'pearl', 'peasant', 'pebble', 'pelmet', 'pencil', 'penny', 'pepe', 'peplum', 'peppermint', 'perf', 'performance', 'perry', 'petal', 'petite', 'pg', 'phone', 'phx', 'pinafore', 'pink', 'pinstripe', 'pintucks', 'piper', 'piping', 'pique', 'piqué', 'pktakm', 'pkthnn', 'pktviy', 'placement', 'placket', 'plaid', 'plain', 'play', 'playful', 'pleat', 'pleated', 'pleats', 'plisse', 'plunge', 'plus', 'plush', 'pocket', 'pockets', 'podium', 'pointelle', 'polar', 'polka', 'polo', 'ponte', 'pop', 'poplin', 'popover', 'postgame', 'power', 'prairie', 'prem', 'premium', 'preppy', 'prime', 'prince', 'print', 'printbest', 'printed', 'pro', 'project', 'prussian', 'pt', 'pu', 'puff', 'puffed', 'puffer', 'puffy', 'pull', 'pullover', 'puma', 'punk', 'purple', 'purpose', 'pyramid', 'qualifier', 'quality', 'queenstown', 'quick', 'quik', 'quilt', 'quilted', 'race', 'racer', 'rachel', 'racing', 'raffy', 'raglan', 'rainbow', 'rainshadow', 'rally', 'randomevent', 'range', 'rapid', 'rat', 'rattan', 'raven', 'raw', 'rdet', 'ready', 'recheck', 'recover', 'recovery', 'recycled', 'red', 'reebok', 'reena', 'reflection', 'reflective', 'reflex', 'reg', 'regular', 'relaxed', 'reoparudo', 'research', 'retro', 'revere', 'reversed', 'reversible', 'revitalize', 'revolve', 'rib', 'ribbed', 'ribbon', 'rick', 'rihanna', 'rikki', 'ring', 'ringer', 'rip', 'ripped', 'rise', 'rival', 'rive', 'riviera', 'rl', 'roadside', 'rock', 'roll', 'rolled', 'roller', 'rollsleeve', 'rookie', 'rope', 'rose', 'round', 'rowan', 'rpd', 'ruched', 'ruffle', 'ruffled', 'ruffles', 'rugby', 'run', 'runa', 'running', 'runway', 'rush', 'rust', 'ryley', 'sable', 'sadie', 'sand', 'sandleford', 'sandra', 'sapphire', 'sateen', 'satin', 'scallop', 'scarab', 'scarf', 'schiffli', 'schino', 'scoop', 'scotch', 'script', 'scuba', 'sculpted', 'sd', 'seam', 'seamfree', 'seamless', 'seams', 'seasons', 'seersucker', 'select', 'self', 'selvedge', 'semi', 'sequin', 'serge', 'series', 'service', 'set', 'sew', 'shacket', 'shaker', 'shale', 'shawl', 'shearling', 'sheath', 'sheer', 'shell', 'sherpa', 'shift', 'shimmer', 'shirred', 'shirt', 'shirts', 'short', 'shorts', 'shots', 'shoulder', 'shrunk', 'signature', 'silk', 'silver', 'simple', 'singapore', 'single', 'singlet', 'siri', 'size', 'skater', 'skinny', 'skirt', 'skirts', 'skort', 'skorts', 'skyline', 'slate', 'sleek', 'sleeping', 'sleeve', 'sleeveless', 'sleeves', 'sless', 'slh', 'slhthom', 'slice', 'slim', 'slimfit', 'slinky', 'slip', 'slit', 'slits', 'slogan', 'slub', 'smart', 'smock', 'smocked', 'smocking', 'snake', 'snakeskin', 'sofia', 'soft', 'solid', 'sophia', 'space', 'spacer', 'spaghetti', 'sparkle', 'special', 'speed', 'speedpocket', 'spicy', 'spinner', 'spirit', 'splater', 'spliced', 'split', 'sport', 'sports', 'sportstyle', 'sportswear', 'spot', 'spots', 'spray', 'spring', 'sprint', 'square', 'sskcslm1', 'stack', 'stadium', 'stamos', 'stand', 'standard', 'star', 'stargazer', 'stars', 'statement', 'stay', 'stefani', 'steffi', 'stellar', 'steven', 'stewie', 'stitch', 'stitching', 'stone', 'storm', 'story', 'straight', 'strap', 'streak', 'streaker', 'stretch', 'stretchable', 'stride', 'string', 'stripe', 'striped', 'stripes', 'structure', 'structured', 'studio', 'style', 'stylish', 'suede', 'suit', 'suite', 'sullivan', 'summer', 'sun', 'sundays', 'sunset', 'super', 'supernova', 'superstate', 'support', 'supremium', 'surf', 'surplice', 'sw', 'sweat', 'sweater', 'sweaters', 'sweatpant', 'sweatpants', 'sweatshirt', 'sweet', 'swim', 'swing', 'swiss', 'swoosh', 't7', 'tab', 'tabs', 'tag', 'tail', 'tailored', 'taiwan', 'talia', 'tall', 'tamara', 'tan', 'tank', 'tape', 'taped', 'taper', 'tapered', 'tara', 'tassel', 'tassle', 'tattersol', 'te', 'teagan', 'teal', 'tech', 'technical', 'technique', 'ted', 'teddy', 'tee', 'tellia', 'temp', 'tencel', 'tennis', 'terrain', 'terry', 'text', 'texture', 'textured', 'thea', 'theo', 'thermo', 'thread', 'threadborne', 'thunder', 'ticket', 'tie', 'tied', 'tiekimono', 'tier', 'tiered', 'tiger', 'tight', 'tights', 'tilly', 'tim', 'timeless', 'times', 'tina', 'tipped', 'tjm', 'toga', 'tokyo', 'tommy', 'tonal', 'tone', 'tortoise', 'towel', 'tower', 'tpr', 'tr', 'track', 'trackpants', 'tracksuit', 'tracktop', 'training', 'travel', 'treats', 'trefoil', 'trench', 'triangle', 'tricot', 'trim', 'trims', 'triple', 'tropical', 'tropics', 'trouser', 'trousers', 'truc', 'trucker', 'true', 'trumpet', 'ts', 'tshirt', 'tt', 'tube', 'tunic', 'turn', 'turtle', 'turtleneck', 'tweed', 'twill', 'twin', 'twist', 'twister', 'tyler', 'type', 'ua', 'ul', 'ultimate', 'ultra', 'unbrushed', 'uneven', 'unisex', 'unstoppable', 'upf', 'utility', 'van', 'vanish', 'vans', 'varied', 'varsity', 'vector', 'vegan', 'velocity', 'velvet', 'venice', 'vent', 'versatile', 'vesper', 'vest', 'victory', 'vie', 'vin', 'vincey', 'vintage', 'vinyl', 'viq', 'viscose', 'visionary', 'visions', 'viva', 'volley', 'vrct', 'w2', 'waffle', 'waist', 'waisted', 'walk', 'walker', 'wall', 'walter', 'warm', 'warning', 'wars', 'wash', 'washed', 'waterfall', 'waterproof', 'wavy', 'way', 'waytt', 'wb', 'wcc', 'weave', 'webb', 'webex', 'westa', 'wet', 'wg', 'white', 'whm', 'wide', 'wild', 'wind', 'windcheater', 'windjacket', 'windtrekker', 'wine', 'winners', 'winter', 'withslits', 'wolter', 'woman', 'women', 'womens', 'wool', 'woolen', 'wordmark', 'work', 'workout', 'wots', 'woven', 'wrap', 'wrapped', 'yarn', 'year', 'years', 'yellow', 'yhan', 'yoga', 'yoke', 'yvette', 'zalora', 'zaria', 'zebra', 'zip', 'ziphood', 'ziphoodie', 'zipped', 'zipper', 'zone', 'ztay']\n"
     ]
    }
   ],
   "source": [
    "# settings that you use for count vectorizer will go here\n",
    "tfidf_vectorizer=TfidfVectorizer(use_idf=True, \n",
    "                                 stop_words=\"english\", \n",
    "                                 analyzer='word', \n",
    "                                 ngram_range=(1, 1))\n",
    "\n",
    "docs = map(''.join, fashion_df[['description']].values.tolist())\n",
    "\n",
    "# just send in all your docs here\n",
    "tfidf_vectorizer_vectors = tfidf_vectorizer.fit_transform(docs)\n",
    "\n",
    "print('Length of vocab: ', len(tfidf_vectorizer.get_feature_names()))\n",
    "print('Sample of vocab', tfidf_vectorizer.get_feature_names()[:]) # Sample of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing and Split data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data to Train and Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80% Training 20% Validation\n",
    "_X_train, _X_test, y_train, y_test = train_test_split(fashion_df['description'], fashion_df['category'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Include TFIDF to our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 4751)\n",
      "(400, 4751)\n"
     ]
    }
   ],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(1, 2), \n",
    "                       stop_words='english', \n",
    "                       analyzer='word')\n",
    "vect.fit(_X_train)\n",
    "\n",
    "# _X_train and _X_test  --- vectorized ---> X_train and X_test\n",
    "X_train = vect.transform(_X_train)\n",
    "X_test = vect.transform(_X_test)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training using Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FEATURES = 2000\n",
    "MAX_LENGTH = 20\n",
    "EMBED_SIZE = 20\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4751"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(256,input_shape=(X_train.shape[1],),activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(128,activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(10,activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\",metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_69 (Dense)             (None, 256)               1216512   \n",
      "_________________________________________________________________\n",
      "dropout_51 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_52 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,250,698\n",
      "Trainable params: 1,250,698\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "y_train_ = le.fit_transform(y_train)\n",
    "y_train__ = tf.keras.utils.to_categorical(y_train_)\n",
    "y_train__  # DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1360 samples, validate on 240 samples\n",
      "Epoch 1/25\n",
      "1360/1360 [==============================] - 1s 442us/step - loss: 2.3002 - acc: 0.1294 - val_loss: 2.2905 - val_acc: 0.2500\n",
      "Epoch 2/25\n",
      "1360/1360 [==============================] - 0s 291us/step - loss: 2.2753 - acc: 0.2912 - val_loss: 2.2705 - val_acc: 0.3125\n",
      "Epoch 3/25\n",
      "1360/1360 [==============================] - 1s 390us/step - loss: 2.2397 - acc: 0.3684 - val_loss: 2.2385 - val_acc: 0.3292\n",
      "Epoch 4/25\n",
      "1360/1360 [==============================] - 0s 335us/step - loss: 2.1799 - acc: 0.4581 - val_loss: 2.1856 - val_acc: 0.4250\n",
      "Epoch 5/25\n",
      "1360/1360 [==============================] - 0s 352us/step - loss: 2.0851 - acc: 0.5434 - val_loss: 2.1051 - val_acc: 0.4708\n",
      "Epoch 6/25\n",
      "1360/1360 [==============================] - 0s 350us/step - loss: 1.9365 - acc: 0.6875 - val_loss: 1.9878 - val_acc: 0.5583\n",
      "Epoch 7/25\n",
      "1360/1360 [==============================] - 0s 283us/step - loss: 1.7321 - acc: 0.7588 - val_loss: 1.8291 - val_acc: 0.5875\n",
      "Epoch 8/25\n",
      "1360/1360 [==============================] - 0s 234us/step - loss: 1.4727 - acc: 0.8103 - val_loss: 1.6398 - val_acc: 0.5958\n",
      "Epoch 9/25\n",
      "1360/1360 [==============================] - 0s 304us/step - loss: 1.1621 - acc: 0.8816 - val_loss: 1.4441 - val_acc: 0.6542\n",
      "Epoch 10/25\n",
      "1360/1360 [==============================] - 0s 239us/step - loss: 0.8792 - acc: 0.9199 - val_loss: 1.2613 - val_acc: 0.6667\n",
      "Epoch 11/25\n",
      "1360/1360 [==============================] - 0s 267us/step - loss: 0.6280 - acc: 0.9434 - val_loss: 1.1088 - val_acc: 0.7083\n",
      "Epoch 12/25\n",
      "1360/1360 [==============================] - 0s 233us/step - loss: 0.4381 - acc: 0.9647 - val_loss: 0.9996 - val_acc: 0.7208\n",
      "Epoch 13/25\n",
      "1360/1360 [==============================] - 0s 260us/step - loss: 0.3210 - acc: 0.9654 - val_loss: 0.9265 - val_acc: 0.7333\n",
      "Epoch 14/25\n",
      "1360/1360 [==============================] - 0s 264us/step - loss: 0.2290 - acc: 0.9794 - val_loss: 0.8768 - val_acc: 0.7458\n",
      "Epoch 15/25\n",
      "1360/1360 [==============================] - 0s 307us/step - loss: 0.1713 - acc: 0.9853 - val_loss: 0.8419 - val_acc: 0.7333\n",
      "Epoch 16/25\n",
      "1360/1360 [==============================] - 0s 259us/step - loss: 0.1280 - acc: 0.9882 - val_loss: 0.8287 - val_acc: 0.7458\n",
      "Epoch 17/25\n",
      "1360/1360 [==============================] - 0s 258us/step - loss: 0.1113 - acc: 0.9860 - val_loss: 0.8130 - val_acc: 0.7458\n",
      "Epoch 18/25\n",
      "1360/1360 [==============================] - 0s 312us/step - loss: 0.0944 - acc: 0.9860 - val_loss: 0.8029 - val_acc: 0.7458\n",
      "Epoch 19/25\n",
      "1360/1360 [==============================] - 0s 291us/step - loss: 0.0707 - acc: 0.9904 - val_loss: 0.8003 - val_acc: 0.7333\n",
      "Epoch 20/25\n",
      "1360/1360 [==============================] - 0s 285us/step - loss: 0.0659 - acc: 0.9926 - val_loss: 0.7984 - val_acc: 0.7375\n",
      "Epoch 21/25\n",
      "1360/1360 [==============================] - 0s 239us/step - loss: 0.0574 - acc: 0.9912 - val_loss: 0.7945 - val_acc: 0.7500\n",
      "Epoch 22/25\n",
      "1360/1360 [==============================] - 0s 362us/step - loss: 0.0498 - acc: 0.9926 - val_loss: 0.7924 - val_acc: 0.7500\n",
      "Epoch 23/25\n",
      "1360/1360 [==============================] - 0s 269us/step - loss: 0.0501 - acc: 0.9926 - val_loss: 0.7931 - val_acc: 0.7500\n",
      "Epoch 24/25\n",
      "1360/1360 [==============================] - 0s 284us/step - loss: 0.0488 - acc: 0.9897 - val_loss: 0.7965 - val_acc: 0.7417\n",
      "Epoch 25/25\n",
      "1360/1360 [==============================] - 0s 305us/step - loss: 0.0386 - acc: 0.9912 - val_loss: 0.7970 - val_acc: 0.7375\n"
     ]
    }
   ],
   "source": [
    "models = model.fit(X_train, y_train__,\n",
    "                    batch_size=256,\n",
    "                    epochs=25,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
