{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "UT3GnHJaEziz",
    "outputId": "1899439d-e215-47f7-e1e6-a742aeb7fc1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.2.0-cp36-cp36m-manylinux2010_x86_64.whl (516.2 MB)\n",
      "Collecting wheel>=0.26; python_version >= \"3\"\n",
      "  Downloading wheel-0.34.2-py2.py3-none-any.whl (26 kB)\n",
      "Collecting protobuf>=3.8.0\n",
      "  Using cached protobuf-3.12.2-cp36-cp36m-manylinux1_x86_64.whl (1.3 MB)\n",
      "Collecting google-pasta>=0.1.8\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting h5py<2.11.0,>=2.10.0\n",
      "  Using cached h5py-2.10.0-cp36-cp36m-manylinux1_x86_64.whl (2.9 MB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.2.1-py3-none-any.whl (63 kB)\n",
      "Collecting tensorflow-estimator<2.3.0,>=2.2.0\n",
      "  Using cached tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n",
      "Collecting grpcio>=1.8.6\n",
      "  Using cached grpcio-1.29.0-cp36-cp36m-manylinux2010_x86_64.whl (3.0 MB)\n",
      "Collecting scipy==1.4.1; python_version >= \"3\"\n",
      "  Using cached scipy-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (26.1 MB)\n",
      "Processing /home/ec2-user/.cache/pip/wheels/c3/af/84/3962a6af7b4ab336e951b7877dcfb758cf94548bb1771e0679/absl_py-0.9.0-py3-none-any.whl\n",
      "Collecting gast==0.3.3\n",
      "  Using cached gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting numpy<2.0,>=1.16.0\n",
      "  Downloading numpy-1.18.5-cp36-cp36m-manylinux1_x86_64.whl (20.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 20.1 MB 7.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard<2.3.0,>=2.2.0\n",
      "  Using cached tensorboard-2.2.2-py3-none-any.whl (3.0 MB)\n",
      "Processing /home/ec2-user/.cache/pip/wheels/32/42/7f/23cae9ff6ef66798d00dc5d659088e57dbba01566f6c60db63/wrapt-1.12.1-cp36-cp36m-linux_x86_64.whl\n",
      "Collecting six>=1.12.0\n",
      "  Using cached six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting keras-preprocessing>=1.1.0\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting astunparse==1.6.3\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Processing /home/ec2-user/.cache/pip/wheels/93/2a/eb/e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc/termcolor-1.1.0-py3-none-any.whl\n",
      "Collecting setuptools\n",
      "  Downloading setuptools-47.3.1-py3-none-any.whl (582 kB)\n",
      "\u001b[K     |████████████████████████████████| 582 kB 42.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.2.2-py3-none-any.whl (88 kB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "\u001b[K     |████████████████████████████████| 298 kB 65.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth<2,>=1.6.3\n",
      "  Using cached google_auth-1.17.2-py2.py3-none-any.whl (90 kB)\n",
      "Collecting requests<3,>=2.21.0\n",
      "  Using cached requests-2.23.0-py2.py3-none-any.whl (58 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.6.0.post3-py3-none-any.whl (777 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\n",
      "Collecting importlib-metadata; python_version < \"3.8\"\n",
      "  Downloading importlib_metadata-1.6.1-py2.py3-none-any.whl (31 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Using cached cachetools-4.1.0-py3-none-any.whl (10 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3\"\n",
      "  Downloading rsa-4.6-py3-none-any.whl (47 kB)\n",
      "\u001b[K     |████████████████████████████████| 47 kB 6.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting idna<3,>=2.5\n",
      "  Using cached idna-2.9-py2.py3-none-any.whl (58 kB)\n",
      "Collecting chardet<4,>=3.0.2\n",
      "  Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Using cached urllib3-1.25.9-py2.py3-none-any.whl (126 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2020.4.5.2-py2.py3-none-any.whl (157 kB)\n",
      "\u001b[K     |████████████████████████████████| 157 kB 68.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.1.0-py3-none-any.whl (4.9 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "\u001b[31mERROR: awscli 1.18.39 has requirement rsa<=3.5.0,>=3.1.2, but you'll have rsa 4.6 which is incompatible.\u001b[0m\n",
      "Installing collected packages: wheel, six, setuptools, protobuf, google-pasta, numpy, h5py, opt-einsum, tensorflow-estimator, grpcio, scipy, absl-py, gast, zipp, importlib-metadata, markdown, werkzeug, pyasn1, pyasn1-modules, cachetools, rsa, google-auth, idna, chardet, urllib3, certifi, requests, tensorboard-plugin-wit, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard, wrapt, keras-preprocessing, astunparse, termcolor, tensorflow\n",
      "Successfully installed absl-py-0.9.0 astunparse-1.6.3 cachetools-4.1.0 certifi-2020.4.5.2 chardet-3.0.4 gast-0.3.3 google-auth-1.17.2 google-auth-oauthlib-0.4.1 google-pasta-0.2.0 grpcio-1.29.0 h5py-2.10.0 idna-2.9 importlib-metadata-1.6.1 keras-preprocessing-1.1.2 markdown-3.2.2 numpy-1.18.5 oauthlib-3.1.0 opt-einsum-3.2.1 protobuf-3.12.2 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.6 scipy-1.4.1 setuptools-47.3.1 six-1.15.0 tensorboard-2.2.2 tensorboard-plugin-wit-1.6.0.post3 tensorflow-2.2.0 tensorflow-estimator-2.2.0 termcolor-1.1.0 urllib3-1.25.9 werkzeug-1.0.1 wheel-0.34.2 wrapt-1.12.1 zipp-3.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow -t ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PFppAGwlE9QO"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "Hj_lRwSVfll2",
    "outputId": "687291df-febd-4934-fc80-3dcb78ef56a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pip in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (20.1.1)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (4.46.1)\n",
      "Requirement already satisfied: numpy==1.17.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (1.17.4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.17.4'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Pip Installation\n",
    "!pip install --upgrade pip\n",
    "!pip install tqdm\n",
    "!pip install numpy==1.17.4\n",
    "\n",
    "np.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5o0x2LsYHnVr"
   },
   "source": [
    "## Test the file directory of google drive with dummy csv\n",
    "not used since this is the sagemaker version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "EvopVDVaGga3",
    "outputId": "9c137595-0ffd-4597-e2da-917b979db0b2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/SageMaker/brainhack2020'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rfMFntMpGafu"
   },
   "outputs": [],
   "source": [
    "#a = pd.read_csv(\"./gdrive/My Drive/datasets/dummy.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "BM2aUBUxGpfL",
    "outputId": "a138e73f-213c-4d48-b0d7-15c7321000e2"
   },
   "outputs": [],
   "source": [
    "#a.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k9y4D5IfHuVI"
   },
   "source": [
    "## Actual CV Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RouIPsTCHwma"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import PIL\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from math import log, exp\n",
    "from tqdm import tqdm\n",
    "from random import shuffle\n",
    "from PIL import ImageEnhance, ImageFont, ImageDraw\n",
    "from IPython.display import Image, display\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.python.keras.utils.data_utils import Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7-rrTqzDITkg"
   },
   "outputs": [],
   "source": [
    "cat_list = ['tops', 'trousers', 'outerwear', 'dresses', 'skirts']\n",
    "\n",
    "input_shape = (224,224,3)\n",
    "wt_decay = 5e-4\n",
    "dims_list = [(7,7),(14,14)]\n",
    "aspect_ratios = [(1,1), (1,2), (2,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q01wn6aNIV5b"
   },
   "outputs": [],
   "source": [
    "# Set up directory\n",
    "data_folder = '/datasets/'\n",
    "submission_folder = os.path.join(data_folder, 'submissions/')\n",
    "train_imgs_folder = os.path.join(data_folder,'train/', 'train/')\n",
    "train_annotations = os.path.join(data_folder,'train.json')\n",
    "val_imgs_folder = os.path.join(data_folder,'val/','val/')\n",
    "val_annotations = os.path.join(data_folder,'val.json')\n",
    "train_pickle = os.path.join( data_folder, 'train.p/','train.p')\n",
    "val_pickle = os.path.join( data_folder, 'val.p/','val.p')\n",
    "\n",
    "save_model_folder = submission_folder\n",
    "load_model_folder = submission_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "id": "nBuDUwC5IdYn",
    "outputId": "e88a6195-0239-4372-8a89-5ff6c56bcaa8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/datasets/train/train/\n",
      "/datasets/train.json\n",
      "/datasets/val/val/\n",
      "/datasets/val.json\n",
      "/datasets/train.p/train.p\n",
      "/datasets/val.p/val.p\n",
      "/datasets/submissions/\n",
      "/datasets/submissions/\n",
      "/datasets/submissions/\n"
     ]
    }
   ],
   "source": [
    "# Check if the directories are correct\n",
    "print(train_imgs_folder)\n",
    "print(train_annotations)\n",
    "print(val_imgs_folder)\n",
    "print(val_annotations)\n",
    "print(train_pickle)\n",
    "print(val_pickle)\n",
    "print(submission_folder)\n",
    "print(save_model_folder)\n",
    "print(load_model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "U87MX6fsIgC-",
    "outputId": "77b74624-7264-4d9d-a8bb-36a6e57a7494"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FwhKCBvlIyu_"
   },
   "source": [
    "## Data Augmentation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gEB7brBFIjBK"
   },
   "outputs": [],
   "source": [
    "# Helper methods: Computes the boundary of the image that includes all bboxes\n",
    "\n",
    "def compute_reasonable_boundary(labels):\n",
    "    bounds = [ (x-w/2, x+w/2, y-h/2, y+h/2) for _,x,y,w,h in labels]\n",
    "    xmin = min([bb[0] for bb in bounds])\n",
    "    xmax = max([bb[1] for bb in bounds])\n",
    "    ymin = min([bb[2] for bb in bounds])\n",
    "    ymax = max([bb[3] for bb in bounds])\n",
    "    return xmin, xmax, ymin, ymax\n",
    "\n",
    "def aug_horizontal_flip(img, labels):\n",
    "    flipped_labels = []\n",
    "    for c,x,y,w,h in labels:\n",
    "        flipped_labels.append( (c,1-x,y,w,h) )\n",
    "    return img.transpose(PIL.Image.FLIP_LEFT_RIGHT), np.array(flipped_labels)\n",
    "\n",
    "def aug_crop(img, labels):\n",
    "    # Compute bounds such that no boxes are cut out\n",
    "    xmin, xmax, ymin, ymax = compute_reasonable_boundary(labels)\n",
    "    # Choose crop_xmin from [0, xmin]\n",
    "    crop_xmin = max( np.random.uniform() * xmin, 0 )\n",
    "    # Choose crop_xmax from [xmax, 1]\n",
    "    crop_xmax = min( xmax + (np.random.uniform() * (1-xmax)), 1 )\n",
    "    # Choose crop_ymin from [0, ymin]\n",
    "    crop_ymin = max( np.random.uniform() * ymin, 0 )\n",
    "    # Choose crop_ymax from [ymax, 1]\n",
    "    crop_ymax = min( ymax + (np.random.uniform() * (1-ymax)), 1 )\n",
    "    # Compute the \"new\" width and height of the cropped image\n",
    "    crop_w = crop_xmax - crop_xmin\n",
    "    crop_h = crop_ymax - crop_ymin\n",
    "    cropped_labels = []\n",
    "    for c,x,y,w,h in labels:\n",
    "        c_x = (x - crop_xmin) / crop_w\n",
    "        c_y = (y - crop_ymin) / crop_h\n",
    "        c_w = w / crop_w\n",
    "        c_h = h / crop_h\n",
    "        cropped_labels.append( (c,c_x,c_y,c_w,c_h) )\n",
    "\n",
    "    W,H = img.size\n",
    "    # Compute the pixel coordinates and perform the crop\n",
    "    impix_xmin = int(W * crop_xmin)\n",
    "    impix_xmax = int(W * crop_xmax)\n",
    "    impix_ymin = int(H * crop_ymin)\n",
    "    impix_ymax = int(H * crop_ymax)\n",
    "    return img.crop( (impix_xmin, impix_ymin, impix_xmax, impix_ymax) ), np.array( cropped_labels )\n",
    "\n",
    "def aug_translate(img, labels):\n",
    "    # Compute bounds such that no boxes are cut out\n",
    "    xmin, xmax, ymin, ymax = compute_reasonable_boundary(labels)\n",
    "    trans_range_x = [-xmin, 1 - xmax]\n",
    "    tx = trans_range_x[0] + (np.random.uniform() * (trans_range_x[1] - trans_range_x[0]))\n",
    "    trans_range_y = [-ymin, 1 - ymax]\n",
    "    ty = trans_range_y[0] + (np.random.uniform() * (trans_range_y[1] - trans_range_y[0]))\n",
    "\n",
    "    trans_labels = []\n",
    "    for c,x,y,w,h in labels:\n",
    "        trans_labels.append((c,x+tx,y+ty,w,h))\n",
    "\n",
    "    W,H = img.size\n",
    "    tx_pix = int(W * tx)\n",
    "    ty_pix = int(H * ty)\n",
    "    return img.rotate(0, translate=(tx_pix, ty_pix)), np.array( trans_labels )\n",
    "\n",
    "def aug_colorbalance(img, labels, color_factors=[0.2,2.0]):\n",
    "    factor = color_factors[0] + np.random.uniform() * (color_factors[1] - color_factors[0])\n",
    "    enhancer = ImageEnhance.Color(img)\n",
    "    return enhancer.enhance(factor), labels\n",
    "\n",
    "def aug_contrast(img, labels, contrast_factors=[0.2,2.0]):\n",
    "    factor = contrast_factors[0] + np.random.uniform() * (contrast_factors[1] - contrast_factors[0])\n",
    "    enhancer = ImageEnhance.Contrast(img)\n",
    "    return enhancer.enhance(factor), labels\n",
    "\n",
    "def aug_brightness(img, labels, brightness_factors=[0.2,2.0]):\n",
    "    factor = brightness_factors[0] + np.random.uniform() * (brightness_factors[1] - brightness_factors[0])\n",
    "    enhancer = ImageEnhance.Brightness(img)\n",
    "    return enhancer.enhance(factor), labels\n",
    "\n",
    "def aug_sharpness(img, labels, sharpness_factors=[0.2,10.0]):\n",
    "    factor = sharpness_factors[0] + np.random.uniform() * (sharpness_factors[1] - sharpness_factors[0])\n",
    "    enhancer = ImageEnhance.Sharpness(img)\n",
    "    return enhancer.enhance(factor), labels\n",
    "\n",
    "# Performs no augmentations and returns the original image and bbox. Used for the validation images.\n",
    "def aug_identity(pil_img, label_arr):\n",
    "    return np.array(pil_img), label_arr\n",
    "\n",
    "# This is the default augmentation scheme that we will use for each training image.\n",
    "def aug_default(img, labels, p={'flip':0.5, 'crop':0.5, 'translate':0.5, 'color':0.2, 'contrast':0.2, 'brightness':0.2, 'sharpness':0.2}):\n",
    "    if p['color'] > np.random.uniform():\n",
    "        img, labels = aug_colorbalance(img, labels)\n",
    "    if p['contrast'] > np.random.uniform():\n",
    "        img, labels = aug_contrast(img, labels)\n",
    "    if p['brightness'] > np.random.uniform():\n",
    "        img, labels = aug_brightness(img, labels)\n",
    "    if p['sharpness'] > np.random.uniform():\n",
    "        img, labels = aug_sharpness(img, labels)\n",
    "    if p['flip'] > np.random.uniform():\n",
    "        img, labels = aug_horizontal_flip(img, labels)\n",
    "    if p['crop'] > np.random.uniform():\n",
    "        img, labels = aug_crop(img, labels)\n",
    "    if p['translate'] > np.random.uniform():\n",
    "        img, labels = aug_translate(img, labels)\n",
    "    return np.array(img), labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DD-rtvHTI0pS"
   },
   "source": [
    "## Custom Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1srd_S6OIlNk"
   },
   "outputs": [],
   "source": [
    "# Shape of ypred: ( batch, i, j, aspect_ratios, 7 ). For a batch,i,j, we get #aspect_ratios vectors of length 7.\n",
    "# Shape of ytrue: ( batch, i, j, aspect_ratios, 9 ). For a batch,i,j, we get #aspect_ratios vectors of length 9 (two more for objectness and cat/loc indicators)\n",
    "def custom_loss(ytrue, ypred):\n",
    "    obj_loss_weight = 1.0\n",
    "    cat_loss_weight = 1.0\n",
    "    loc_loss_weight = 1.0\n",
    "\n",
    "    end_cat = len(cat_list) + 1\n",
    "\n",
    "    objloss_indicators = ytrue[:,:,:,:,-2:-1]\n",
    "    catlocloss_indicators = ytrue[:,:,:,:,-1:]\n",
    "\n",
    "    ytrue_obj, ypred_obj = ytrue[:,:,:,:,:1], ypred[:,:,:,:,:1]\n",
    "    ytrue_obj = tf.where( objloss_indicators != 0, ytrue_obj, 0 )\n",
    "    ypred_obj = tf.where( objloss_indicators != 0, ypred_obj, 0 )\n",
    "    objectness_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)( ytrue_obj, ypred_obj )\n",
    "\n",
    "    ytrue_cat, ypred_cat = ytrue[:,:,:,:,1:end_cat], ypred[:,:,:,:,1:end_cat]\n",
    "    ytrue_cat = tf.where( catlocloss_indicators != 0, ytrue_cat, 0 )\n",
    "    ypred_cat = tf.where( catlocloss_indicators != 0, ypred_cat, 0 )\n",
    "    categorical_loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True) ( ytrue_cat, ypred_cat )\n",
    "\n",
    "    # Remember that ytrue is longer than ypred, so we will need to stop at index -2, which is where the indicators are stored\n",
    "    ytrue_loc, ypred_loc = ytrue[:,:,:,:,end_cat:-2], ypred[:,:,:,:,end_cat:]\n",
    "    ytrue_loc = tf.where( catlocloss_indicators != 0, ytrue_loc, 0 )\n",
    "    ypred_loc = tf.where( catlocloss_indicators != 0, ypred_loc, 0 )\n",
    "    localisation_loss = tf.keras.losses.Huber() ( ytrue_loc, ypred_loc )\n",
    "\n",
    "    return obj_loss_weight*objectness_loss + cat_loss_weight*categorical_loss + loc_loss_weight*localisation_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P22FJ9JNI6Lv"
   },
   "source": [
    "## IOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yJNl2i-SIn7D"
   },
   "outputs": [],
   "source": [
    "# Computes the intersection-over-union (IoU) of two bounding boxes\n",
    "def iou(bb1, bb2):\n",
    "    x1,y1,w1,h1 = bb1\n",
    "    xmin1 = x1 - w1/2\n",
    "    xmax1 = x1 + w1/2\n",
    "    ymin1 = y1 - h1/2\n",
    "    ymax1 = y1 + h1/2\n",
    "\n",
    "    x2,y2,w2,h2 = bb2\n",
    "    xmin2 = x2 - w2/2\n",
    "    xmax2 = x2 + w2/2\n",
    "    ymin2 = y2 - h2/2\n",
    "    ymax2 = y2 + h2/2\n",
    "\n",
    "    area1 = w1*h1\n",
    "    area2 = w2*h2\n",
    "\n",
    "    # Compute the boundary of the intersection\n",
    "    xmin_int = max(xmin1, xmin2)\n",
    "    xmax_int = min(xmax1, xmax2)\n",
    "    ymin_int = max(ymin1, ymin2)\n",
    "    ymax_int = min(ymax1, ymax2)\n",
    "    intersection = max(xmax_int - xmin_int, 0) * max(ymax_int - ymin_int, 0)\n",
    "\n",
    "    # Remove the double counted region\n",
    "    union = area1+area2-intersection\n",
    "\n",
    "    return intersection / union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SCf8Z3IzJAeZ"
   },
   "source": [
    "## Sampling Schemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "98QIHd94IrKL"
   },
   "outputs": [],
   "source": [
    "# Sampling schemes\n",
    "def yolo_posneg_sampling(iou_scores_dict, label_tensor, gtclass, cat_list, iou_threshold=0.5):\n",
    "    iou_scores = []\n",
    "    for _, scores in iou_scores_dict.items():\n",
    "        iou_scores.extend(scores)\n",
    "    iou_scores.sort( key=lambda x: x[0], reverse=True )\n",
    "  \n",
    "    top_iou_score = iou_scores.pop(0)\n",
    "    _, key, i, j, k, dx, dy, dw, dh = top_iou_score\n",
    "    zeros = [0] * len(cat_list)\n",
    "    payload = [1, *zeros, dx,dy,dw,dh]\n",
    "    payload[gtclass + 1] = 1\n",
    "    # Train objectness, class and loc for the positive\n",
    "    label_tensor[key][i,j,k,-2:] = 1\n",
    "    label_tensor[key][i,j,k,:len(payload)] = payload\n",
    "\n",
    "    # Train objectness only for the negatives\n",
    "    low_iou_scores = [iou_score for iou_score in iou_scores if iou_score[0] < iou_threshold]\n",
    "    for _, key, i, j, k, _, _, _, _ in low_iou_scores:\n",
    "        label_tensor[key][i,j,k,-2] = 1\n",
    "\n",
    "def modified_yolo_posneg_sampling(iou_scores_dict, label_tensor, gtclass, cat_list, iou_threshold=0.5):\n",
    "    iou_scores = []\n",
    "    zeros = [0] * len(cat_list)\n",
    "\n",
    "    for _, scores in iou_scores_dict.items():\n",
    "        iou_scores.extend(scores)\n",
    "    iou_scores.sort( key=lambda x: x[0], reverse=True )\n",
    "  \n",
    "    top_iou_score = iou_scores.pop(0)\n",
    "    _, key, i, j, k, dx, dy, dw, dh = top_iou_score\n",
    "    payload = [1, *zeros, dx,dy,dw,dh]\n",
    "    payload[gtclass + 1] = 1\n",
    "    # Train objectness, class and loc for the positive\n",
    "    label_tensor[key][i,j,k,-2:] = 1\n",
    "    label_tensor[key][i,j,k,:len(payload)] = payload\n",
    "\n",
    "    # Train objectness only for the negatives\n",
    "    low_iou_scores = [iou_score for iou_score in iou_scores if iou_score[0] < iou_threshold]\n",
    "    for _, key, i, j, k, _, _, _, _ in low_iou_scores:\n",
    "        label_tensor[key][i,j,k,-2] = 1\n",
    "\n",
    "    # Train cat/loc only for the in-betweens - those with high IoU but not positive\n",
    "    high_iou_scores = [iou_score for iou_score in iou_scores if iou_score[0] >= iou_threshold]\n",
    "    for _, key, i, j, k, dx, dy, dw, dh in high_iou_scores:\n",
    "        label_tensor[key][i,j,k,-1] = 1\n",
    "        payload = [0,*zeros,dx,dy,dw,dh]\n",
    "        payload[gtclass + 1] = 1\n",
    "        label_tensor[key][i,j,k,:len(payload)] = payload\n",
    "\n",
    "def top_ratio_sampling(iou_scores_dict, label_tensor, gtclass, cat_list, positive_ratio=0.25):\n",
    "    iou_scores = []\n",
    "    # Let all tensors learn objectness score\n",
    "    for v in label_tensor.values():\n",
    "        v[:,:,:,-2] = 1\n",
    "  \n",
    "    for _, iou_score_list in iou_scores_dict.items():\n",
    "        iou_score_list.sort( key=lambda x: x[0], reverse=True )\n",
    "        top_percentile_iou_scores = iou_score_list[:round(len(iou_score_list) * positive_ratio)]\n",
    "        # Include the rest that cross the IoU threshold\n",
    "        iou_score_list = top_percentile_iou_scores + [iou_score for iou_score in iou_score_list[len(top_percentile_iou_scores):] if iou_score[0] >= self.iou_threshold]\n",
    "        iou_scores.extend( iou_score_list )\n",
    "\n",
    "    for iou_score in iou_scores:\n",
    "        IoU, key, i, j, k, dx, dy, dw, dh = iou_score\n",
    "        zeros = [0] * len(cat_list)\n",
    "        payload = [IoU, *zeros, dx,dy,dw,dh]\n",
    "        payload[gtclass + 1] = 1\n",
    "        label_tensor[key][i,j,k,:len(payload)] = payload\n",
    "        # Set the classification/localisation indicator at this location to positive\n",
    "        label_tensor[key][i,j,k,-1] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j48ueWUTJJI2"
   },
   "source": [
    "## Encoding labels/ Decoding model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eBO__QLLJKWA"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Encoder: label -> tensor\n",
    "label_arr: np array like:\n",
    "[[class_idx x y w h]]: num_labels x 5\n",
    "...  \n",
    "Used to figure out for each label line, which tensor entry to shove it into.\n",
    "If the box corresponding to the tensor entry overlaps the ground truth by at least a predefined threshold, then we shove it in.\n",
    "'''\n",
    "\n",
    "def encode_label(label_arr, dims_list, aspect_ratios, iou_fn, sampling_fn, cat_list):\n",
    "    num_entries = 7 + len(cat_list) # objectness, ... len(cat_list) ..., dx, dy, dw, dh, obj_indicator, catloc_indicator\n",
    "    np_labels = {}\n",
    "    for dims in dims_list:\n",
    "        dimkey = '{}x{}'.format(*dims)\n",
    "        np_labels[dimkey] = np.zeros( (*dims, len(aspect_ratios), num_entries ) )\n",
    "\n",
    "    for label in label_arr:\n",
    "        gtclass, gtx, gty, gtw, gth = label\n",
    "        gtclass = int(gtclass)\n",
    "        gt_bbox = [gtx, gty, gtw, gth]\n",
    "    \n",
    "        iou_scores_dict = {}\n",
    "\n",
    "        for dims in dims_list:\n",
    "            key = '{}x{}'.format(*dims)\n",
    "    \n",
    "            kx,ky = dims\n",
    "            gapx = 1.0 / kx\n",
    "            gapy = 1.0 / ky\n",
    "            \n",
    "\n",
    "            # There are kx x ky tiles. \n",
    "            # For now, all have the same w,h of gapx,gapy. \n",
    "            # For the (i,j)-th tile, x = 0.5*gapx + i*gapx = (0.5+i)*gapx | y = (0.5+j)*gapy\n",
    "            \n",
    "            for i in range(kx):\n",
    "                for j in range(ky):\n",
    "                    for k in range( len(aspect_ratios) ):\n",
    "                        dims_aspect_key = (*dims, k) # a 3-tuple: (dim1,dim2,ar)\n",
    "                        if dims_aspect_key not in iou_scores_dict:\n",
    "                            iou_scores_dict[dims_aspect_key] = []\n",
    "                        x = (0.5+i)*gapx\n",
    "                        y = (0.5+j)*gapy\n",
    "\n",
    "                        # Different aspect ratios alter the anchor box default dimensions\n",
    "                        w = gapx * aspect_ratios[k][0]\n",
    "                        h = gapy * aspect_ratios[k][1]\n",
    "                        cand_bbox = [x,y,w,h]\n",
    "\n",
    "                        # SSD formulation\n",
    "                        dx = (gtx - x) / w \n",
    "                        dy = (gty - y) / h\n",
    "                        dw = log( gtw / w )\n",
    "                        dh = log( gth / h )\n",
    "            \n",
    "                        int_over_union = iou_fn( cand_bbox, gt_bbox )\n",
    "                        iou_scores_dict[dims_aspect_key].append( (int_over_union, key, i, j, k, dx, dy, dw, dh) )\n",
    "            sampling_fn( iou_scores_dict, np_labels, gtclass, cat_list )\n",
    "    return np_labels\n",
    "\n",
    "def decode_tensor(pred_dict, aspect_ratios):\n",
    "    results = []\n",
    "    for dim_str, pred_tensor in pred_dict.items():\n",
    "        pred_tensor = pred_tensor[0] # remove the batch\n",
    "        kx, ky = [int(g) for g in dim_str.split('x')]\n",
    "        gapx = 1. / kx\n",
    "        gapy = 1. / ky\n",
    "\n",
    "        # We trained without activations, so we need to process the logits into probabilities/scores\n",
    "        pred_arr = np.array(pred_tensor)\n",
    "        obj_logits = pred_arr[:,:,:,0]\n",
    "        obj_scores = 1. / (1 + np.exp(-obj_logits))\n",
    "        pred_arr[:,:,:,0] = obj_scores\n",
    "\n",
    "        cls_logits = pred_arr[:,:,:,1:-4]\n",
    "        cls_scores = np.exp(cls_logits)\n",
    "        cls_scores = cls_scores / cls_scores.sum(axis=-1)[...,np.newaxis]\n",
    "        pred_arr[:,:,:,1:-4] = cls_scores\n",
    "\n",
    "        for k, ar in enumerate(aspect_ratios):\n",
    "            for i in range(kx):\n",
    "                for j in range(ky):\n",
    "                    cx = (0.5+i)*gapx\n",
    "                    cy = (0.5+j)*gapy\n",
    "                    w = gapx * ar[0]\n",
    "                    h = gapy * ar[1]\n",
    "\n",
    "                    payload = pred_arr[i,j,k]\n",
    "                    obj_score = payload[0]\n",
    "                    dx, dy, dw, dh = payload[-4:]\n",
    "                    cls_probs = payload[1:-4]\n",
    "\n",
    "                    predx = (dx * w) + cx\n",
    "                    predy = (dy * h) + cy\n",
    "                    predw = w * exp( dw )\n",
    "                    predh = h * exp( dh )\n",
    "                    max_cls_idx = np.argmax( cls_probs )\n",
    "                    max_cls_prob = cls_probs[max_cls_idx]\n",
    "                    category_id = max_cls_idx + 1\n",
    "                    det_score = obj_score * max_cls_prob\n",
    "                    results.append( (det_score, category_id, predx, predy, predw, predh) )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IhMy4TlYJW4j"
   },
   "source": [
    "## Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CZ_snVQlJROm"
   },
   "outputs": [],
   "source": [
    "class TILSequence(Sequence):\n",
    "\n",
    "    def __init__(self, img_folder, json_annotation_file, batch_size, augment_fn, input_size, label_encoder, preprocess_fn, testmode=False):\n",
    "        self._prepare_data(img_folder, json_annotation_file)\n",
    "        self.batch_size = batch_size\n",
    "        self.augment_fn = augment_fn\n",
    "        self.input_wh = (*input_size[:2][::-1],input_size[2])\n",
    "        self.label_encoder = label_encoder\n",
    "        self.preprocess_fn = preprocess_fn\n",
    "        self.testmode = testmode\n",
    "    \n",
    "    def _prepare_data(self, img_folder, json_annotation_file):\n",
    "        imgs_dict = {im.split('.')[0]:im for im in os.listdir(img_folder) if im.endswith('.jpg')}\n",
    "        data_dict = {}\n",
    "        with open(json_annotation_file, 'r') as f:\n",
    "            annotations_dict = json.load(f)\n",
    "        annotations_list = annotations_dict['annotations']\n",
    "        for annotation in annotations_list:\n",
    "            img_id = str(annotation['image_id'])\n",
    "            c = annotation['category_id'] - 1 # TODO: make sure that category ids start from 1, not 0\n",
    "            boxleft,boxtop,boxwidth,boxheight = annotation['bbox']\n",
    "            if img_id in imgs_dict:\n",
    "                img_fp = os.path.join(img_folder, imgs_dict[img_id])\n",
    "                imwidth,imheight = PIL.Image.open(img_fp).size\n",
    "                if img_id not in data_dict:\n",
    "                    data_dict[img_id] = []\n",
    "                box_cenx = boxleft + boxwidth/2.\n",
    "                box_ceny = boxtop + boxheight/2.\n",
    "                x,y,w,h = box_cenx/imwidth, box_ceny/imheight, boxwidth/imwidth, boxheight/imheight\n",
    "\n",
    "                data_dict[img_id].append( [c,x,y,w,h] )\n",
    "        self.x, self.y, self.ids = [], [], []\n",
    "        for img_id, labels in data_dict.items():\n",
    "            self.x.append( os.path.join(img_folder, imgs_dict[img_id]) )\n",
    "            self.y.append( np.array(labels) )\n",
    "            self.ids.append( img_id )\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "        return self.get_batch_test(idx) if self.testmode else self.get_batch(idx)\n",
    "\n",
    "    def get_batch_test(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_ids = self.ids[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "\n",
    "        x_acc, y_acc = [], {}\n",
    "        original_img_dims = []\n",
    "        with Pool(self.batch_size) as p:\n",
    "            # Read in the PIL objects from filepaths\n",
    "            batch_x = p.map(load_img, batch_x)\n",
    "    \n",
    "        for x,y in zip( batch_x, batch_y ):\n",
    "            W,H = x.size\n",
    "            original_img_dims.append( (W,H) )\n",
    "            x_aug, y_aug = self.augment_fn( x, y )\n",
    "            if x_aug.size != self.input_wh[:2]:\n",
    "                x_aug.resize( self.input_wh )\n",
    "            x_acc.append( np.array(x_aug) )\n",
    "            y_dict = self.label_encoder( y_aug )\n",
    "            for dimkey, label in y_dict.items():\n",
    "                if dimkey not in y_acc:\n",
    "                    y_acc[dimkey] = []\n",
    "                y_acc[dimkey].append( label )\n",
    "\n",
    "        return batch_ids, original_img_dims, self.preprocess_fn( np.array( x_acc ) ), { dimkey: np.array( gt_tensor ) for dimkey, gt_tensor in y_acc.items() }\n",
    "\n",
    "    def get_batch(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "\n",
    "        x_acc, y_acc = [], {}\n",
    "        with Pool(self.batch_size) as p:\n",
    "            # Read in the PIL objects from filepaths\n",
    "            batch_x = p.map(load_img, batch_x)\n",
    "\n",
    "        for x,y in zip( batch_x, batch_y ):\n",
    "            x_aug, y_aug = self.augment_fn( x, y )\n",
    "            if x_aug.size != self.input_wh[:2]:\n",
    "                x_aug.resize( self.input_wh )\n",
    "            x_acc.append( np.array(x_aug) )\n",
    "            y_dict = self.label_encoder( y_aug )\n",
    "            for dimkey, label in y_dict.items():\n",
    "                if dimkey not in y_acc:\n",
    "                    y_acc[dimkey] = []\n",
    "                y_acc[dimkey].append( label )\n",
    "\n",
    "        return self.preprocess_fn( np.array( x_acc ) ), { dimkey: np.array( gt_tensor ) for dimkey, gt_tensor in y_acc.items() }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7_s5AA9mJZui"
   },
   "outputs": [],
   "source": [
    "class TILPickle(Sequence):\n",
    "    def __init__(self, pickle_file, batch_size, augment_fn, input_size, label_encoder, preprocess_fn, testmode=False):\n",
    "        with open(pickle_file, 'rb') as p:\n",
    "            self.ids, self.x, self.y = pickle.load(p)\n",
    "        self.batch_size = batch_size\n",
    "        self.augment_fn = augment_fn\n",
    "        self.input_wh = (*input_size[:2][::-1],input_size[2])\n",
    "        self.label_encoder = label_encoder\n",
    "        self.preprocess_fn = preprocess_fn\n",
    "        self.testmode = testmode\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_ids = self.ids[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "\n",
    "        x_acc, y_acc = [], {}\n",
    "    \n",
    "        for x,y in zip( batch_x, batch_y ):\n",
    "            x_aug, y_aug = self.augment_fn( x, y )\n",
    "            if x_aug.size != self.input_wh[:2]:\n",
    "                x_aug.resize( self.input_wh )\n",
    "            x_acc.append( np.array(x_aug) )\n",
    "            y_dict = self.label_encoder( y_aug )\n",
    "            for dimkey, label in y_dict.items():\n",
    "                if dimkey not in y_acc:\n",
    "                    y_acc[dimkey] = []\n",
    "                y_acc[dimkey].append(label)\n",
    "\n",
    "        if self.testmode:\n",
    "            return batch_ids, self.preprocess_fn( np.array( x_acc ) ), { dimkey: np.array( gt_tensor ) for dimkey, gt_tensor in y_acc.items() }\n",
    "        return self.preprocess_fn( np.array( x_acc ) ), { dimkey: np.array( gt_tensor ) for dimkey, gt_tensor in y_acc.items() }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6wxIQJBBJ-0u"
   },
   "source": [
    "## Constructing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fM0J24l-JcAg"
   },
   "outputs": [],
   "source": [
    "def transfer_model(backbone_model, input_shape, dims_list, num_aspect_ratios, wt_decay, model_name='transfer-objdet-model'):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    backbone_output = backbone_model(inputs) #7\n",
    "\n",
    "    x = layers.Conv2D(512, 1, padding='same', kernel_regularizer=l2(wt_decay))(backbone_output) #7\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.01)(x)\n",
    "    x = layers.Conv2D(512, 3, padding='valid', kernel_regularizer=l2(wt_decay))(x) #5\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.01)(x)\n",
    "    x = layers.Conv2D(256, 1, padding='same', kernel_regularizer=l2(wt_decay))(x) #5\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.01)(x)\n",
    "    x = layers.Conv2D(512, 3, padding='valid', kernel_regularizer=l2(wt_decay))(x) #3\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.01)(x)\n",
    "\n",
    "    x = layers.Conv2D(256, 1, padding='same', kernel_regularizer=l2(wt_decay))(x) #3\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.01)(x)\n",
    "    x = layers.Conv2D(512, 3, padding='same', kernel_regularizer=l2(wt_decay))(x) #3\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.01)(x)\n",
    "\n",
    "    # You can accumulate more scales via shortcut. Imagine each (n,m) is a grid super-imposed on the original image.\n",
    "    # See the next cell for an example for more scales.\n",
    "    dim_tensor_map = {'3x3': x}\n",
    "\n",
    "    # For each dimension, construct a predictions tensor. Accumulate them into a dictionary for keras to understand multiple labels.\n",
    "    preds_dict = {}\n",
    "    for dims in dims_list:\n",
    "        dimkey = '{}x{}'.format(*dims)\n",
    "        tens = dim_tensor_map[dimkey]\n",
    "        ar_preds = []\n",
    "        for _ in range(num_aspect_ratios):\n",
    "            objectness_preds = layers.Conv2D(1, 1, kernel_regularizer=l2(wt_decay))( tens )\n",
    "            class_preds = layers.Conv2D(len(cat_list), 1, kernel_regularizer=l2(wt_decay))( tens )\n",
    "            bbox_preds = layers.Conv2D(4, 1, kernel_regularizer=l2(wt_decay))( tens )\n",
    "            ar_preds.append( layers.Concatenate()([objectness_preds, class_preds, bbox_preds]) )\n",
    "\n",
    "        if num_aspect_ratios > 1:\n",
    "            predictions = layers.Concatenate()(ar_preds)\n",
    "        elif num_aspect_ratios == 1:\n",
    "            predictions = ar_preds[0]\n",
    "    \n",
    "        predictions = layers.Reshape( (*dims, num_aspect_ratios, 5+len(cat_list)), name=dimkey )(predictions)\n",
    "        preds_dict[dimkey] = predictions\n",
    "\n",
    "    model = keras.Model(inputs, preds_dict, name=model_name)\n",
    "\n",
    "    model.compile( optimizer=tf.keras.optimizers.Adam(1e-5),\n",
    "                 loss=custom_loss )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cbA9GtEgJ_c6"
   },
   "outputs": [],
   "source": [
    "def transfer_model_7x7_14x14(backbone_model, input_shape, dims_list, num_aspect_ratios, wt_decay, model_name='transfer-objdet-model-7x7-14x14'):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    intermediate_layer_model = keras.Model(inputs=backbone_model.input,\n",
    "                                         outputs=backbone_model.get_layer('conv4_block6_out').output)\n",
    "    intermediate_output = intermediate_layer_model(inputs) #14\n",
    "    backbone_output = backbone_model(inputs) #7\n",
    "\n",
    "    x = layers.Conv2D(512, 1, padding='same', kernel_regularizer=l2(wt_decay))(backbone_output) #7\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.01)(x)\n",
    "    x = layers.Conv2D(1024, 3, padding='same', kernel_regularizer=l2(wt_decay))(x) #7\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.01)(x)\n",
    "    x = layers.Conv2D(512, 1, padding='same', kernel_regularizer=l2(wt_decay))(x) #7\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.01)(x)\n",
    "    x = layers.Conv2D(1024, 3, padding='same', kernel_regularizer=l2(wt_decay))(x) #7\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.01)(x)\n",
    "    x = layers.Conv2D(512, 1, padding='same', kernel_regularizer=l2(wt_decay))(x) #7\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    upsample = layers.LeakyReLU(0.01)(x)\n",
    "    x = layers.Conv2D(2048, 3, padding='same', kernel_regularizer=l2(wt_decay))(upsample) #7\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.01)(x)\n",
    "    tens_7x7 = layers.Add()([x,backbone_output])\n",
    "\n",
    "    x = layers.Conv2D(256, 1, padding='same', kernel_regularizer=l2(wt_decay))(upsample) #7\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.01)(x)\n",
    "    x = layers.Conv2DTranspose(512, 5, strides=(2, 2), padding='same')(x) #14\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.01)(x)\n",
    "\n",
    "    x = layers.Concatenate()([x,intermediate_output])\n",
    "\n",
    "    x = layers.Conv2D(256, 1, padding='same', kernel_regularizer=l2(wt_decay))(x) #14\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.01)(x)\n",
    "    x = layers.Conv2D(512, 3, padding='same', kernel_regularizer=l2(wt_decay))(x) #14\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.01)(x)\n",
    "    x = layers.Conv2D(256, 1, padding='same', kernel_regularizer=l2(wt_decay))(x) #14\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.01)(x)\n",
    "    x = layers.Conv2D(512, 3, padding='same', kernel_regularizer=l2(wt_decay))(x) #14\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.01)(x)\n",
    "    x = layers.Conv2D(256, 1, padding='same', kernel_regularizer=l2(wt_decay))(x) #14\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.01)(x)\n",
    "    x = layers.Conv2D(512, 3, padding='same', kernel_regularizer=l2(wt_decay))(x) #14\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    tens_14x14 = layers.LeakyReLU(0.01)(x)\n",
    "\n",
    "    dim_tensor_map = {'7x7': tens_7x7, '14x14': tens_14x14}\n",
    "\n",
    "    # For each dimension, construct a predictions tensor. Accumulate them into a dictionary for keras to understand multiple labels.\n",
    "    preds_dict = {}\n",
    "    for dims in dims_list:\n",
    "        dimkey = '{}x{}'.format(*dims)\n",
    "        tens = dim_tensor_map[dimkey]\n",
    "        ar_preds = []\n",
    "        for _ in range(num_aspect_ratios):\n",
    "            objectness_preds = layers.Conv2D(1, 1, kernel_regularizer=l2(wt_decay))( tens )\n",
    "            class_preds = layers.Conv2D(len(cat_list), 1, kernel_regularizer=l2(wt_decay))( tens )\n",
    "            bbox_preds = layers.Conv2D(4, 1, kernel_regularizer=l2(wt_decay))( tens )\n",
    "            ar_preds.append( layers.Concatenate()([objectness_preds, class_preds, bbox_preds]) )\n",
    "\n",
    "        if num_aspect_ratios > 1:\n",
    "            predictions = layers.Concatenate()(ar_preds)\n",
    "        elif num_aspect_ratios == 1:\n",
    "            predictions = ar_preds[0]\n",
    "    \n",
    "        predictions = layers.Reshape( (*dims, num_aspect_ratios, 5+len(cat_list)), name=dimkey )(predictions)\n",
    "        preds_dict[dimkey] = predictions\n",
    "\n",
    "    model = keras.Model(inputs, preds_dict, name=model_name)\n",
    "\n",
    "    model.compile( optimizer=tf.keras.optimizers.Adam(1e-5),\n",
    "                 loss=custom_loss )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "sut7VnL2KEVK",
    "outputId": "b14ffeca-b32c-4d5f-c39d-56a1aadbcd22"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The input must have 3 channels; got `input_shape=(224, 224, 3)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-f2cd8581791a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mload_model_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mbackbone_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResNet50\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransfer_model_7x7_14x14\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackbone_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdims_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdims_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_aspect_ratios\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maspect_ratios\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwt_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwt_decay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_context\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'-res50'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SageMaker/brainhack2020/tensorflow/python/keras/applications/resnet.py\u001b[0m in \u001b[0;36mResNet50\u001b[0;34m(include_top, weights, input_tensor, input_shape, pooling, classes, **kwargs)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m   return ResNet(stack_fn, False, True, 'resnet50', include_top, weights,\n\u001b[0;32m--> 463\u001b[0;31m                 input_tensor, input_shape, pooling, classes, **kwargs)\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SageMaker/brainhack2020/tensorflow/python/keras/applications/resnet.py\u001b[0m in \u001b[0;36mResNet\u001b[0;34m(stack_fn, preact, use_bias, model_name, include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m       \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_data_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m       \u001b[0mrequire_flatten\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude_top\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m       weights=weights)\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0minput_tensor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SageMaker/brainhack2020/tensorflow/python/keras/applications/imagenet_utils.py\u001b[0m in \u001b[0;36mobtain_input_shape\u001b[0;34m(input_shape, default_size, min_size, data_format, require_flatten, weights)\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'imagenet'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m           raise ValueError('The input must have 3 channels; got '\n\u001b[0;32m--> 345\u001b[0;31m                            '`input_shape=' + str(input_shape) + '`')\n\u001b[0m\u001b[1;32m    346\u001b[0m         if ((input_shape[1] is not None and input_shape[1] < min_size) or\n\u001b[1;32m    347\u001b[0m             (input_shape[2] is not None and input_shape[2] < min_size)):\n",
      "\u001b[0;31mValueError\u001b[0m: The input must have 3 channels; got `input_shape=(224, 224, 3)`"
     ]
    }
   ],
   "source": [
    "# Choose whether to start a new model \n",
    "# or load a previously trained one\n",
    "model_context = 'model-7x7-14x14-3aspect-modyoloposneg-wd{}'.format(wt_decay)\n",
    "\n",
    "#load_model_path = os.path.join( load_model_folder, '{}-best_val_loss.h5'.format(model_context) )\n",
    "load_model_path = None\n",
    "\n",
    "if load_model_path is None:\n",
    "    backbone_model = tf.keras.applications.ResNet50(input_shape=input_shape, include_top=False)\n",
    "    model = transfer_model_7x7_14x14(backbone_model, input_shape=input_shape, dims_list=dims_list, num_aspect_ratios=len(aspect_ratios), wt_decay=wt_decay, model_name=model_context+'-res50')\n",
    "else:\n",
    "    model = tf.keras.models.load_model(load_model_path, custom_objects={'custom_loss':custom_loss})\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B-8zEFSUKWhq"
   },
   "source": [
    "## Training/Transfer Learning of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4W9qNyKBKIFQ"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "- There is overfitting now that I set top 25% (of each dim-ar combination) as positives. How?\n",
    "- Larger image size - maybe 448\n",
    "- Transfer learning\n",
    "- Change weights of losses?\n",
    "\n",
    "# Also add more callbacks, such as tensorboard \n",
    "dataset, batch_size, augment_fn, input_size, label_encoder, preprocess_fn\n",
    "encode_label(label_arr, dims_list, aspect_ratios, iou_fn, sampling_fn, cat_list)\n",
    "img_folder, json_annotation_file, batch_size, augment_fn, input_size, label_encoder, preprocess_fn\n",
    "'''\n",
    "\n",
    "bs=16\n",
    "n_epochs_warmup = 30\n",
    "n_epochs_after = 70\n",
    "\n",
    "label_encoder = lambda y: encode_label(y, dims_list, aspect_ratios, iou, modified_yolo_posneg_sampling, cat_list)\n",
    "preproc_fn = lambda x: x / 255.\n",
    "\n",
    "print('Creating training sequence...')\n",
    "train_sequence = TILSequence(train_imgs_folder, train_annotations, bs, aug_default, input_shape, label_encoder, preproc_fn)\n",
    "train_sequence = TILPickle(train_pickle, bs, aug_default, input_shape, label_encoder, preproc_fn)\n",
    "print('Creating validation sequence...')\n",
    "val_sequence = TILSequence(val_imgs_folder, val_annotations, bs, aug_identity, input_shape, label_encoder, preproc_fn)\n",
    "val_sequence = TILPickle(val_pickle, bs, aug_identity, input_shape, label_encoder, preproc_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SRBSZy-iKXmh"
   },
   "outputs": [],
   "source": [
    "save_model_path = os.path.join(save_model_folder, '{}-best_val_loss.h5'.format(model_context))\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "                                                                filepath=save_model_path,\n",
    "                                                                save_weights_only=False,\n",
    "                                                                monitor='val_loss',\n",
    "                                                                mode='auto',\n",
    "                                                                save_best_only=True)\n",
    "\n",
    "earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=30)\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-8)\n",
    "\n",
    "for layer in backbone_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "7ykhJC9EKi9R",
    "outputId": "9ce78464-8ed6-43a0-94d7-b2f8339347cc"
   },
   "outputs": [],
   "source": [
    "print('Warming up the model...')\n",
    "model.fit(x=train_sequence, \n",
    "          epochs=n_epochs_warmup, \n",
    "          validation_data=val_sequence,\n",
    "          callbacks=[model_checkpoint_callback, earlystopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "v8R3xiNAKlSX",
    "outputId": "3563781b-5443-4bb2-c45c-63ccdb2e47b9"
   },
   "outputs": [],
   "source": [
    "# Fine tuning\n",
    "print('Model warmed. Loading best val version of model...')\n",
    "load_model_path = os.path.join( load_model_folder, '{}-best_val_loss.h5'.format(model_context) )\n",
    "del model\n",
    "model = tf.keras.models.load_model(load_model_path, custom_objects={'custom_loss':custom_loss})\n",
    "\n",
    "for layer in model.get_layer('resnet50').layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-5), loss=custom_loss)\n",
    "model_context = 'ft-' + model_context\n",
    "save_model_path = os.path.join( save_model_folder, '{}-best_val_loss.h5'.format(model_context) )\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "                                                                filepath=save_model_path,\n",
    "                                                                save_weights_only=False,\n",
    "                                                                monitor='val_loss',\n",
    "                                                                mode='auto',\n",
    "                                                                save_best_only=True)\n",
    "earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=30)\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-8)\n",
    "\n",
    "model.fit(x=train_sequence, \n",
    "          epochs=n_epochs_after, \n",
    "          validation_data=val_sequence, \n",
    "          callbacks=[model_checkpoint_callback, earlystopping, reduce_lr])\n",
    "\n",
    "# Final save\n",
    "model.save(os.path.join(save_model_folder, '{}-final.h5'.format(model_context)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VtzLZ4MruJVM"
   },
   "source": [
    "## Non-max suppression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rz2w3a_il9ba"
   },
   "outputs": [],
   "source": [
    "# To fix multiple, we introduce non-maximum suppression, or NMS for short\n",
    "def nms(detections, iou_thresh=0.):\n",
    "  dets_by_class = {}\n",
    "  final_result = []\n",
    "  for det in detections:\n",
    "    cls = det[1]\n",
    "    if cls not in dets_by_class:\n",
    "      dets_by_class[cls] = []\n",
    "    dets_by_class[cls].append( det )\n",
    "  for _, dets in dets_by_class.items():\n",
    "    candidates = list(dets)\n",
    "    candidates.sort( key=lambda x:x[0], reverse=True )\n",
    "    while len(candidates) > 0:\n",
    "      candidate = candidates.pop(0)\n",
    "      _,_,cx,cy,cw,ch = candidate\n",
    "      copy = list(candidates)\n",
    "      for other in candidates:\n",
    "        # Compute the IoU. If it exceeds thresh, we remove it\n",
    "        _,_,ox,oy,ow,oh = other\n",
    "        if iou( (cx,cy,cw,ch), (ox,oy,ow,oh) ) > iou_thresh:\n",
    "          copy.remove(other)\n",
    "      candidates = list(copy)\n",
    "      final_result.append(candidate)\n",
    "  return final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DG7zDN-PuTo6"
   },
   "source": [
    "## Load a pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "NBvbAWSVuRDs",
    "outputId": "cb71795d-4e66-4fd9-d062-94a418d05de4"
   },
   "outputs": [],
   "source": [
    "# load the model\n",
    "load_model_path = os.path.join( load_model_folder, 'model-7x7-14x14-3aspect-modyoloposneg-wd0.0005-best_val_loss.h5' )\n",
    "model = tf.keras.models.load_model(load_model_path, custom_objects={'custom_loss':custom_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "85U566AJuZZJ"
   },
   "outputs": [],
   "source": [
    "# load the test data\n",
    "label_encoder = lambda y: encode_label(y, dims_list, aspect_ratios, iou, modified_yolo_posneg_sampling, cat_list)\n",
    "preproc_fn = lambda x: x / 255.\n",
    "\n",
    "test_sequence_pickle = TILPickle(val_pickle, 1, aug_identity, input_shape, label_encoder, preproc_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y9ZYrvJyWNwO"
   },
   "outputs": [],
   "source": [
    "test_sequence = TILSequence(val_imgs_folder, val_annotations, 1, aug_identity, input_shape, label_encoder, preproc_fn, testmode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V_42ECKouX4y"
   },
   "source": [
    "## Visualize Model Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "v93eWhNZud3P",
    "outputId": "854d9867-85c8-4702-f305-ff26d615bfe5"
   },
   "outputs": [],
   "source": [
    "# Run this to visualize\n",
    "rank_colors = ['cyan', 'magenta', 'pink']\n",
    "det_threshold=0.\n",
    "top_dets=3\n",
    "\n",
    "start=0\n",
    "end=20\n",
    "for k in range(start,end):\n",
    "  img_arr, label_cxywh = test_sequence_pickle[k]\n",
    "  img_arr = img_arr[0]\n",
    "  pil_img = PIL.Image.fromarray( (img_arr * 255.).astype(np.uint8) )\n",
    "  W,H = pil_img.size\n",
    "  pred_dict = model(np.array([img_arr]))\n",
    "  preds = decode_tensor( pred_dict, aspect_ratios )\n",
    "    \n",
    "  # Post-processing\n",
    "  preds.sort( key=lambda x:x[0], reverse=True )\n",
    "  preds = [pred for pred in preds if pred[0] >= det_threshold]\n",
    "  preds = preds[:top_dets]\n",
    "  preds = nms(preds, iou_thresh=0.5)\n",
    "\n",
    "  draw_img = pil_img.copy()\n",
    "  draw = ImageDraw.Draw(draw_img)\n",
    "  for i, pred in enumerate(preds):\n",
    "    conf,cls,x,y,w,h = pred\n",
    "    bb_x = int(x * W)\n",
    "    bb_y = int(y * H)\n",
    "    bb_w = int(w * W)\n",
    "    bb_h = int(h * H)\n",
    "    left = int(bb_x - bb_w / 2)\n",
    "    top = int(bb_y - bb_h / 2)\n",
    "    right = int(bb_x + bb_w / 2)\n",
    "    bot = int(bb_y + bb_h / 2)\n",
    "    cls_str = cat_list[cls-1]\n",
    "\n",
    "    draw.rectangle(((left, top), (right, bot)), outline=rank_colors[i])\n",
    "    draw.text((bb_x, bb_y), cls_str, fill=rank_colors[i])\n",
    "    draw.text( ( int(left + bb_w*.1), int(top + bb_h*.1) ), '{:.2f}'.format(conf), fill=rank_colors[i] )\n",
    "\n",
    "  display(draw_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3IWZgZULSJWV"
   },
   "source": [
    "## Generating detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "6Xu57IkMR1jK",
    "outputId": "7df1b201-8df8-4895-c096-4a3a601b3fc3"
   },
   "outputs": [],
   "source": [
    "# Generating detections on the folder of validation images\n",
    "detections = []\n",
    "det_threshold=0.\n",
    "for i in tqdm(range(len(test_sequence))):\n",
    "  img_id, dims, input_arr, _ = test_sequence[i]\n",
    "  img_id = int(img_id[0])\n",
    "  W,H = dims[0]\n",
    "\n",
    "  # Here, I'm inferencing one-by-one, but you can batch it if you want it faster\n",
    "  pred_dict = model(input_arr)\n",
    "  preds = decode_tensor( pred_dict, aspect_ratios )\n",
    "\n",
    "  # Post-processing\n",
    "  preds = [pred for pred in preds if pred[0] >= det_threshold]\n",
    "  preds.sort( key=lambda x:x[0], reverse=True )\n",
    "  preds = preds[:100] # we only evaluate you on 100 detections per image\n",
    "\n",
    "  for i, pred in enumerate(preds):\n",
    "    conf,cat_id,x,y,w,h = pred\n",
    "    left = W * (x - w/2.)\n",
    "    left = round(left,1)\n",
    "    top = H * (y - h/2.)\n",
    "    top = round(top,1)\n",
    "    width = W*w\n",
    "    width = round(width,1)\n",
    "    height = H*h\n",
    "    height = round(height,1)\n",
    "    conf = float(conf)\n",
    "    cat_id = int(cat_id)\n",
    "    detections.append( {'image_id':img_id, 'category_id':cat_id, 'bbox':[left, top, width, height], 'score':conf} )\n",
    "\n",
    "with open('detections-7x7-14x14-top100-nonms.json', 'w') as f:\n",
    "  json.dump(detections, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "colab_type": "code",
    "id": "iMOqbqfkSJKO",
    "outputId": "4f6b1757-ccef-488a-97dc-a9d6de0cdedb"
   },
   "outputs": [],
   "source": [
    "#This installation is a modified version of the original to suit this competition\n",
    "! pip install git+https://github.com/jinmingteo/cocoapi.git#subdirectory=PythonAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FnEFeuvUdiKK"
   },
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 482
    },
    "colab_type": "code",
    "id": "amv4RY1Ndo2H",
    "outputId": "8997b833-cabc-46cb-958f-d32ac4d2bb0a"
   },
   "outputs": [],
   "source": [
    "# Get evaluation score against validation set\n",
    "coco_gt = COCO(val_annotations)\n",
    "coco_dt = coco_gt.loadRes('./gdrive/My Drive/datasets/detections-7x7-14x14-top100-nonms.json')\n",
    "cocoEval = COCOeval(cocoGt=coco_gt, cocoDt=coco_dt, iouType='bbox')\n",
    "cocoEval.evaluate()\n",
    "cocoEval.accumulate()\n",
    "cocoEval.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IczmnrU8hF3s"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UJ2xmDV6drOQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CV.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
